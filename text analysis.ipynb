{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "from operator import itemgetter\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"data/overview_wikipedia_sample.json\", lines=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_overview(dirty_overview):\n",
    "    # remove the <ref> </ref>\n",
    "    overview = re.sub('<ref.*</ref>', '', dirty_overview)\n",
    "    overview = re.sub('<ref.*/>', '', overview)\n",
    "\n",
    "    # remove {{ }} and what is inside\n",
    "    overview = re.sub('[\\{].*[\\}]', '', overview)\n",
    "    \n",
    "    # reomve [[File: ]] and [[Image: ]] and what is inside\n",
    "    overview = re.sub('\\[\\[File:.*?\\]\\]', '', overview)\n",
    "    overview = re.sub('\\[\\[Image:.*?\\]\\]', '', overview)\n",
    "\n",
    "    # remove [[ ]] and keep what is inside and for the cases like [[abc | def]] keep only def and remove the rest\n",
    "    overview = re.sub(r'\\[\\[(?:[^\\]|]*\\|)?([^\\]|]*)\\]\\]', r'\\1', overview)\n",
    "\n",
    "    # remove ''' ''' \n",
    "    overview = re.sub('\\'{2,3}', '', overview)\n",
    "\n",
    "    # remove \\n\n",
    "    overview = re.sub('\\n', '', overview)\n",
    "    \n",
    "    return overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.overview = df.overview.map(lambda x: clear_overview(x))\n",
    "df.iloc[0].overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_adjective(token):\n",
    "    if not token.is_stop:\n",
    "        if token.pos_ == 'ADJ':\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adjectives(overview):\n",
    "    doc = nlp(overview)\n",
    "    adjs = [token.lemma_ for token in doc if is_adjective(token)]\n",
    "    return adjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['adjectives'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['adjectives'] = df.overview.map(lambda x: get_adjectives(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.adjectives.str.len() > 5].adjectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(overview)\n",
    "\n",
    "#strings are encoded to hashes\n",
    "tokens = [token.text for token in doc]\n",
    "\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_stop_words_tokens = [token for token in doc if not token.is_stop]\n",
    "\n",
    "non_stop_words_tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PoS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tagged = [(token.text, token.pos_) for token in non_stop_words_tokens]\n",
    "\n",
    "pos_tagged[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjectives = [token[0] for token in pos_tagged if token[1] == 'ADJ']\n",
    "\n",
    "adjectives[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [token.lemma_ for token in doc if is_adjective(token)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# five most common tokens\n",
    "word_freq = Counter(words)\n",
    "common_words = word_freq.most_common()\n",
    "\n",
    "print(common_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DICTIONARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjectivity_lexicon = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/subjectivity_lexicon.tff\", \"r\") as file:\n",
    "    for line in file:\n",
    "        elements = line.split(\" \")\n",
    "        word = elements[2][6:]\n",
    "        subjectivity_lexicon[word] = (elements[0][5:], elements[5][14:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjectivity_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in df.iloc[24].adjectives:\n",
    "    try:\n",
    "        print(a, subjectivity_lexicon[a])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada] *",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
